{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X5Yox0gG1DsF"
   },
   "source": [
    "Homework 5: Neural Language Models  (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 4\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names & Sections\n",
    "----\n",
    "Names: Julia Geller (4120) & Shae Marks (4120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxWlOU9k1DsQ"
   },
   "source": [
    "Task 4: Compare your generated sentences (15 points)\n",
    "----\n",
    "\n",
    "In this task, you'll analyze one of the files that you produced in Task 3. You'll need to compare against the corresponding file that we have provided for you that was generated from the vanilla n-gram language model.\n",
    "\n",
    "Choose *__one__* of the following two options.\n",
    "\n",
    "Option 1: Evaluate the generated words of *character*-based models\n",
    "---\n",
    "\n",
    "Your job for this option is to programmatically measure two things:\n",
    "1. the percentage of words produced by each model that are valid english words.\n",
    "2. the percentage of words produced by each model that are valid english words *and* were not seen at train time.\n",
    "\n",
    "For this task, a word is defined as \"characters between _ \" or \"characters between spaces\" (if you replaced your underscores with spaces when you printed out your new sentences).\n",
    "\n",
    "\n",
    "Make sure to turn in any necessary supporting files along with your submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your imports here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How did you determine what a valid english word is? __YOUR ANSWER HERE__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Gather the sequences of characters that are determined not to be words. Sampling at minimum 100 of these sequences, how many of them *should have* been counted as words in your opinion?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more code here, as needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit two csv files alongside this notebook: `valid_words_lms.csv` and `invalid_words_lms.csv`. Both files should have __two__ columns: `model`, `sequence`. `model` will have the value `neural` or `vanilla`. `sequence` will be the corresponding sequence of characters. `valid_words_lms.csv` should contain all sequences from both models you determined to be valid words. `invalid_words_lms.csv` will have all sequences from both models you programatically determined to be invalid words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "316H0xSh1DsQ"
   },
   "source": [
    "Option 2: Evaluate the generated sentences of *word*-based models\n",
    "----\n",
    "\n",
    "Your job for this option is to measure the quality of your generated sentences for word-based models. For this option you *must* survey at least 3 people who are __not__ in this course. They need to speak and read the language that you are evaluating, but they need not be native speakers.\n",
    "\n",
    "You will evaluate the quality of the generated sentences in the following way:\n",
    "1. Generate 20 sentences from your neural model.\n",
    "2. Using the same level of n-gram, pair these sentences with provided sentences from the vanilla n-gram model.\n",
    "\n",
    "Next, build a survey. For each pair of (neural LM sentence, vanilla n-ngram LM sentence), you'll ask the survey taker two binary selection questions:\n",
    "1. which sentence is more grammatical?\n",
    "2. which sentence makes more sense, semantically (in meaning)?\n",
    "3. Which sentence do you prefer?\n",
    "\n",
    "\n",
    "Finally, you'll evaluate your survey results. Calculate the following:\n",
    "1. What percentage of neural vs. vanilla n-gram LM sentences were preferred, separated along each of the three dimensions?\n",
    "\n",
    "For all questions, the vanilla model was preferred.\n",
    "- For the more grammatical question, the vanilla model was preferred at 61.7%.\n",
    "- For the more sense question, the vanilla model was preferred at 63.3%.\n",
    "- For the preference question, the vanilla model was preferred at 58.3%.\n",
    "2. What is [Krippendorff's alpha](https://en.wikipedia.org/wiki/Krippendorff%27s_alpha) for your survey data? \n",
    "- For the more grammatical question, the alpha was 0.445.\n",
    "- For the more sense question, the alpha was 0.153.\n",
    "- For the preferred question, the alpha was 0.326.\n",
    "\n",
    "\n",
    "You are welcome to use a pre-built python implmenetation of the Krippendorff's alpha calculation, such as [this one](https://pypi.org/project/krippendorff/). Krippendorff's alpha is one way to measure interannotator agreement â€” the extent to which your survey respondants agree with one another.\n",
    "\n",
    "You will submit your survey data alongside this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install krippendorff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your imports here\n",
    "import krippendorff\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "# read in txt files - each line is a sequence \n",
    "\n",
    "def txt_to_array(filepath: str, num_seq: int=20, seed: int=18):\n",
    "    \"\"\"\n",
    "    Reads in the content at the given filepath and produces a list of size num_seq\n",
    "    where each element is a randomly chosen line from the filepath.\n",
    "\n",
    "    Args:\n",
    "    filepath (string): path to a txt file \n",
    "    num_seq (int): number of sequences to choose from the txt file \n",
    "    seed (int): seed to give a consistent result with random library \n",
    "\n",
    "    Returns:\n",
    "    lines (list): list of sequences\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove trailing newline characters from each line\n",
    "    lines = [line.strip() for line in lines]\n",
    "\n",
    "    random.seed(seed)\n",
    "    lines = random.sample(lines, k=num_seq) # sample without replacement \n",
    "\n",
    "    return lines \n",
    "\n",
    "\n",
    "vanilla_ngram_seqs_word = txt_to_array(\"spooky_vanilla_3_word.txt\")\n",
    "neurallm_seqs_word = txt_to_array(\"neurallm_seq_word.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1\n",
      "Vanilla N-Gram: but it continued and habitual ride , i fled from me '' said he , `` this is the massacre of the genuine `` hey , yew , why should he come , the tiger i alternately rattled the rusty impediments with a face and gnashed his teeth in the crumbling bricks and unhealthy weeds as was really the poet .\n",
      "Neural Net: all the five four thousand undiscovered health on a couple lunatic , and it , and i will send me with generous to betray neck would secure : these notes have better existed , or any wave the bulk we shall mad a reply student to die on my extra entreaties of her constancy . \n",
      "\n",
      "Pair 2\n",
      "Vanilla N-Gram: the artificial style of gardening , a new scene of the question , when he approached the arch of the volumes i have said that an advantageous offer of private possession were thrown down , so far evinced not the faces of men , '' ran the text represented an artificial alphabet , notwithstanding all the information it could not find a local door which the next thing that had laid her fair cheek upon his knees in the place , the fourth story were of glazed brick and wood , and that he had lost them both without firing the one , that his long and strange , complicated , but these were notorious , although certainly done to the town ; and no cold eye meet mine to check the impending doom , to take the hint , i possess no distinct and furious whiffs from his arms being fastened in a writing desk were still .\n",
      "Neural Net: this pamphlet the dying had interested her asylum ; but so gently his perfectly brilliant branch pace the frame induced to dare a picture space woods surfaces windows and walls , and , one should endure place under which fear , in two minutes reflection these had been his mother , that the foretopsail are not easily obscurely proof . \n",
      "\n",
      "Pair 3\n",
      "Vanilla N-Gram: he had heard , adding his own .\n",
      "Neural Net: by the mineral wave whereby with me , but a cloak by the breeze from their , and quitting on what golden , in no end in the lovers of the interments element , and made him . \n",
      "\n",
      "Pair 4\n",
      "Vanilla N-Gram: this craving <UNK> strength as i expected the letter was returned to this study would have used .\n",
      "Neural Net: she have not the use of heaven , or buzz as in charity i 'll blessed he was the top of opening ; with the gratitude of our party , by a passage to find his chair to the machine lifted back the walls and long my arms , and whatever in the sea was only backward performed . \n",
      "\n",
      "Pair 5\n",
      "Vanilla N-Gram: when he says they was a discordant melody ; there was something to say that the voice of a bride .\n",
      "Neural Net: full reaching and deep loose key and the reigns material one that he should choose your creator victor how you know you know that he 'll mr. ellison l g want of succeeding that men who found my good of harmless , and the possibility of distinction again as if my brain ; you yourself tell that , nor this ca he had been unpleasantly and day may eve , that central might have been pulled , with a lion . \n",
      "\n",
      "Pair 6\n",
      "Vanilla N-Gram: `` what is passing around you , my own reflections , lord raymond from this height , and the mass of forbidden fruit , for excessive sorrow prevents improvement or exaltation of the fire under the eye of the electric lights began to rattle in a way to ooth nargai had not lost .\n",
      "Neural Net: it was rather so i might separate . \n",
      "\n",
      "Pair 7\n",
      "Vanilla N-Gram: apparently it was the palace with a mountain piled above i care not , gave him a pas de zephyr .\n",
      "Neural Net: he had learnt of raymond had lately done me . \n",
      "\n",
      "Pair 8\n",
      "Vanilla N-Gram: and so floated over the <UNK> of trouble .\n",
      "Neural Net: in he hours gone . \n",
      "\n",
      "Pair 9\n",
      "Vanilla N-Gram: but i did find something which perplexed me in english a language which painted your own free choice .\n",
      "Neural Net: this horde , but you did quickly regarding horribly and adoration they 're n't tell what like loose greater cries which assert would have wronged any idea that he had escaped it upon my hovel isle into the room . \n",
      "\n",
      "Pair 10\n",
      "Vanilla N-Gram: `` you have no power .\n",
      "Neural Net: and did festival out a pale section with motion , and encompassed races that scarcely precipitated its native soul . \n",
      "\n",
      "Pair 11\n",
      "Vanilla N-Gram: she saw that he had asked for the third watched .\n",
      "Neural Net: they again , leading . \n",
      "\n",
      "Pair 12\n",
      "Vanilla N-Gram: i pondered .\n",
      "Neural Net: man to our occupations . \n",
      "\n",
      "Pair 13\n",
      "Vanilla N-Gram: that 's capital that will be comfort for you .\n",
      "Neural Net: as we found the dead both realised ourselves to the agency of subjects ; the repeaters were brief ere however , and does the half old hag were caused sensual of morbid abnormality of her greek is , though it . \n",
      "\n",
      "Pair 14\n",
      "Vanilla N-Gram: but we had that night without end .\n",
      "Neural Net: yet , and gave very restless returned an attendant of passion . \n",
      "\n",
      "Pair 15\n",
      "Vanilla N-Gram: but , in a gentleman , whilst we were a wilderness of ocean .\n",
      "Neural Net: he always not chance o favour . \n",
      "\n",
      "Pair 16\n",
      "Vanilla N-Gram: adrian had received no impressions from the roof is a freak of the nightmare countries .\n",
      "Neural Net: i first endured in the cellar of the earth . \n",
      "\n",
      "Pair 17\n",
      "Vanilla N-Gram: my first step should be disturbed .\n",
      "Neural Net: the feeble plain gable along . \n",
      "\n",
      "Pair 18\n",
      "Vanilla N-Gram: and if i had swooned .\n",
      "Neural Net: with here of which it is dark most unaccountably ones . \n",
      "\n",
      "Pair 19\n",
      "Vanilla N-Gram: upon rising from the shopkeeper `` my travels ; and thither shouldst thou go an thou wouldst not desire against me .\n",
      "Neural Net: the statue began on in a home for it mostly was many feeble long after is any case likely . \n",
      "\n",
      "Pair 20\n",
      "Vanilla N-Gram: briden pushed at the hideous legends i had with me for illustrious achievements . ''\n",
      "Neural Net: to rival upon every of the effects of my little paper extent . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(\"Pair\", i+1)\n",
    "    print(\"Vanilla N-Gram:\", vanilla_ngram_seqs_word[i])\n",
    "    print(\"Neural Net:\", neurallm_seqs_word[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response examples: ['B', 'A', 'A', 'B', 'B']\n"
     ]
    }
   ],
   "source": [
    "# read in survey responses (A is the vanilla ngram model, B is the neural net language model. We removed any <UNK> characters.)\n",
    "survey_df = pd.read_csv('sentence_eval_survey_responses.csv')\n",
    "\n",
    "# save answers to each question in different lists\n",
    "cols = [c for c in survey_df.columns if c not in ['Timestamp', 'Email Address']] # get response columns \n",
    "start = 0\n",
    "q1 = []\n",
    "q2 = []\n",
    "q3 = []\n",
    "\n",
    "# iterate through the 20 sequence pairs \n",
    "for i in range(0, 20):\n",
    "    q1 += survey_df[cols[start:start+3][0]].values.tolist() # responses to question 1 for pair i\n",
    "    q2 += survey_df[cols[start:start+3][1]].values.tolist() # responses to question 2 for pair i\n",
    "    q3 += survey_df[cols[start:start+3][2]].values.tolist() # responses to question 3 for pair i\n",
    "    start += 3\n",
    "\n",
    "\n",
    "print(\"response examples:\", q2[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which sentence is more grammatical?\n",
      "vanilla preference: 0.617  neural network preference: 0.383 \n",
      "\n",
      "Which sentence makes more sense, semantically (in meaning)?\n",
      "vanilla preference: 0.633  neural network preference: 0.367 \n",
      "\n",
      "Which sentence do you prefer?\n",
      "vanilla preference: 0.583  neural network preference: 0.41700000000000004 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see what percent of preferences were for vanilla vs neural network models\n",
    "vanilla = len([v for v in q1 if v=='A'])/len(q1)\n",
    "print(\"Which sentence is more grammatical?\")\n",
    "print('vanilla preference:', round(vanilla,3), ' neural network preference:', 1-round(vanilla,3), \"\\n\")\n",
    "\n",
    "vanilla = len([v for v in q2 if v=='A'])/len(q2)\n",
    "print(\"Which sentence makes more sense, semantically (in meaning)?\")\n",
    "print('vanilla preference:', round(vanilla,3), ' neural network preference:', 1-round(vanilla,3), \"\\n\")\n",
    "\n",
    "vanilla = len([v for v in q3 if v=='A'])/len(q3)\n",
    "print(\"Which sentence do you prefer?\")\n",
    "print('vanilla preference:', round(vanilla,3), ' neural network preference:', 1-round(vanilla,3), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intialize data_reliability for each question\n",
    "d1 = [[], [], []] # [[person1 responses], [person2 responses], [person3 responses]] to question 1 \n",
    "d2 =[[], [], []]\n",
    "d3 = [[], [], []]\n",
    "# add each survey taker's answer to the correct data reliability sublist for each question\n",
    "for i in range(0,60):\n",
    "\n",
    "    if i%3 == 0:\n",
    "        # Survey taker 1\n",
    "        j = 0\n",
    "    elif i%3 == 1:\n",
    "        # Survey taker 2\n",
    "        j=1\n",
    "    else:\n",
    "        # Survey taker 3\n",
    "        j=2\n",
    "        \n",
    "    d1[j].append(q1[i])\n",
    "    d2[j].append(q2[i])\n",
    "    d3[j].append(q3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 alpha: 0.445358401880141\n",
      "Question 2 alpha: 0.15311004784688997\n",
      "Question 3 alpha: 0.32571428571428573\n"
     ]
    }
   ],
   "source": [
    "# print alpha score\n",
    "print('Question 1 alpha:', krippendorff.alpha(reliability_data=d1, level_of_measurement='nominal'))\n",
    "print('Question 2 alpha:', krippendorff.alpha(reliability_data=d2, level_of_measurement='nominal'))\n",
    "print('Question 3 alpha:', krippendorff.alpha(reliability_data=d3, level_of_measurement='nominal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "wordembeddings_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "05041e657fa0436a83611a3d2d345b99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3cd0685004814c0d974a1d809e0e2b4f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_b0dca775977048f38841afae3d906eb6",
      "value": "100%"
     }
    },
    "140057e9712f46af9ebf5825ef9b1390": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_05041e657fa0436a83611a3d2d345b99",
       "IPY_MODEL_a818afa6bb4f43c8b7e32a3c04f17211",
       "IPY_MODEL_72a47718e310461fbd61b312f7bf7cfe"
      ],
      "layout": "IPY_MODEL_488b55855d4d4ffc8af6d3d77aa3fdf8"
     }
    },
    "150adc7de7f54d63a215482e6a977067": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b93060412f54083b6dd7b9203ae55d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cd0685004814c0d974a1d809e0e2b4f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "488b55855d4d4ffc8af6d3d77aa3fdf8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "72a47718e310461fbd61b312f7bf7cfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a4d9e5b3a1e144e6b34a55ab5cbce43f",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_150adc7de7f54d63a215482e6a977067",
      "value": " 19579/19579 [00:00&lt;00:00, 18295.70it/s]"
     }
    },
    "843343b9adc84d949f839d51814d55aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a4d9e5b3a1e144e6b34a55ab5cbce43f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a818afa6bb4f43c8b7e32a3c04f17211": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b93060412f54083b6dd7b9203ae55d0",
      "max": 19579,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_843343b9adc84d949f839d51814d55aa",
      "value": 19579
     }
    },
    "b0dca775977048f38841afae3d906eb6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
