{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5: Neural Language Models  (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Feedforward Neural Language Model (60 points)\n",
    "--------------------------\n",
    "\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shaem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing utility functions from Keras\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# necessary\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# optional\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "# if you want fancy progress bars\n",
    "from tqdm import notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# your other imports here\n",
    "import time\n",
    "import neurallm_utils as nutils\n",
    "from gensim.models import KeyedVectors # imported by me \n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NGRAM = 3 # The ngram language model you want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in necessary data\n",
    "TRAIN_FILE = 'spooky_author_train.csv'\n",
    "char_texts = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)\n",
    "word_texts = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Tokenizer and fit on your data\n",
    "# do this for both the word and character data\n",
    "\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(word_texts)\n",
    "encoded_words = word_tokenizer.texts_to_sequences(word_texts)\n",
    "\n",
    "\n",
    "char_tokenizer = Tokenizer()\n",
    "char_tokenizer.fit_on_texts(char_texts)\n",
    "encoded_chars = char_tokenizer.texts_to_sequences(char_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for character embeddings: 60\n",
      "Vocab size for word embeddings: 25374\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the word index for each of your tokenizers\n",
    "# this should match what you calculated in Task 2 with your embeddings\n",
    "\n",
    "# Vocabulary size for character embeddings is 60\n",
    "# Vocabulary size for word embeddings is 25374\n",
    "print('Vocab size for character embeddings:', len(char_tokenizer.word_index))\n",
    "print('Vocab size for word embeddings:', len(word_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Next, prepare the sequences to train your model from text (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed n-gram based sequences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depening on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t                      y\n",
    "    this,    process                                    however\n",
    "    process, however                                    afforded\n",
    "    however, afforded\t                                me\n",
    "\n",
    "\n",
    "Our first step is to translate the text into sequences of numbers, \n",
    "one sequence per n-gram window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character ngram training samples:\n",
      "Total sequences: 2957553\n",
      "[21, 21, 3]\n",
      "[21, 3, 9]\n",
      "[3, 9, 7]\n",
      "[9, 7, 8]\n",
      "[7, 8, 1]\n",
      "\n",
      "Word ngram training samples:\n",
      "Total sequences: 634080\n",
      "[1, 1, 32]\n",
      "[1, 32, 2956]\n",
      "[32, 2956, 3]\n",
      "[2956, 3, 155]\n",
      "[3, 155, 3]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngram_training_samples(encoded: list, ngram: int) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the ngram training samples out of it.\n",
    "    Parameters:\n",
    "    encoded: a list of lists produced by kera's tokenizer mapping tokens to unique indices \n",
    "    ngram: the size of the ngrams that should be produced \n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    ngram_samples = []\n",
    "\n",
    "    for line in encoded:\n",
    "        for i in range(len(line) - ngram + 1):\n",
    "            ngram_samples.append(line[i:i+ngram])\n",
    "            \n",
    "    return ngram_samples\n",
    "\n",
    "\n",
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "#\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "# [21, 21, 3]\n",
    "# [21, 3, 9]\n",
    "# [3, 9, 7]\n",
    "# ...\n",
    "# Spooky data by words shoud give 634080 sequences\n",
    "# [1, 1, 32]\n",
    "# [1, 32, 2956]\n",
    "# [32, 2956, 3]\n",
    "# ...\n",
    "\n",
    "char_ngrams = generate_ngram_training_samples(encoded_chars, NGRAM)\n",
    "word_ngrams = generate_ngram_training_samples(encoded_words, NGRAM)\n",
    "\n",
    "print(\"Character ngram training samples:\")\n",
    "print(\"Total sequences:\", len(char_ngrams))\n",
    "for i in range(5):\n",
    "    print(char_ngrams[i])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Word ngram training samples:\")\n",
    "print(\"Total sequences:\", len(word_ngrams))\n",
    "for i in range(5):\n",
    "    print(word_ngrams[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X data for words: (634080, 2)\n",
      "Shape of y data for words: (634080,)\n",
      "\n",
      "Shape of X data for characters: (2957553, 2)\n",
      "Shape of y data for characters: (2957553,)\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here\n",
    "def split_ngrams(ngrams: list) -> (np.array, np.array):\n",
    "    \"\"\"\n",
    "    Separate ngram sequences into lists of X and y data.\n",
    "    Args:\n",
    "    ngrams (list of lists): sequences of ngrams in the form of [[x1, x2, ... , x(n-1), y], ...]\n",
    "\n",
    "    Returns:\n",
    "    X (2-D numpy array): sequences of the first n-1 tokens in the ngrams [[x1, x2, ... , x(n-1)], ...]\n",
    "    y (1-D numpy array): list of all the labels aka the n token in the ngrams [y1, y2, ...]\n",
    "    \"\"\"\n",
    "    X =  np.array([ngram[:-1] for ngram in ngrams])\n",
    "    y = np.array([ngram[-1] for ngram in ngrams])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_word, y_word = split_ngrams(word_ngrams)\n",
    "X_char, y_char = split_ngrams(char_ngrams)\n",
    "\n",
    "# print out the shapes to verify that they are correct\n",
    "print(\"Shape of X data for words:\", X_word.shape)\n",
    "print(\"Shape of y data for words:\", y_word.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Shape of X data for characters:\", X_char.shape)\n",
    "print(\"Shape of y data for characters:\", y_char.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Initialize a function that reads the word embeddings you saved earlier\n",
    "# and gives you back mappings from words to their embeddings and also \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def read_embeddings(filename: str, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters:\n",
    "        filename (str): path to file\n",
    "        Tokenizer: tokenizer used to tokenize the data (needed to get the word to index mapping)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # word2vec maps words to embedding vectors \n",
    "    word2vec_embeddings = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "    # initialize dictionaries \n",
    "    token_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "\n",
    "    # tokenizer maps words to unique indices \n",
    "    for token, index in tokenizer.word_index.items():\n",
    "        embedding = word2vec_embeddings[token]\n",
    "\n",
    "        token_to_embedding[token] = embedding\n",
    "        index_to_embedding[index] = embedding\n",
    "\n",
    "    return (token_to_embedding, index_to_embedding)\n",
    "\n",
    "\n",
    "token_to_embedding_words, index_to_embedding_words = read_embeddings(\"spooky_embedding_word.txt\", word_tokenizer)\n",
    "token_to_embedding_chars, index_to_embedding_chars = read_embeddings(\"spooky_embedding_char.txt\", char_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NECESSARY FOR CHARACTERS\n",
    "\n",
    "# the \"0\" index of the Tokenizer is assigned for the padding token. Initialize\n",
    "# the vector for padding token as all zeros of embedding size\n",
    "# this adds one to the number of embeddings that were initially saved\n",
    "# (and increases your vocab size by 1)\n",
    "\n",
    "index_to_embedding_words[0] = [0] * (EMBEDDINGS_SIZE * 2)\n",
    "index_to_embedding_chars[0] = [0] * (EMBEDDINGS_SIZE * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, index_2_embedding: dict) -> (list,list):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "    \n",
    "    If for_feedforward is True: \n",
    "    Returns data generator to be used by feed_forward\n",
    "    else: Returns data generator for RNN model\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # iterate over X and y in batches \n",
    "    for i in range(0, len(X), num_sequences_per_batch):\n",
    "        X_batch = X[i:i+num_sequences_per_batch]\n",
    "\n",
    "        # represents embeddings for each sequence in X \n",
    "        # flattened so resulting shape is (batch_size, (n-1)*EMBEDDING_SIZE)\n",
    "        X_batch_embeddings = []\n",
    "        for X_sequence in X_batch:\n",
    "            # embeddings for a single training sequence - flattened to have length (n-1)*EMBEDDING_SIZE, where n is the number of tokens/indices \n",
    "            X_sequence_embeddings = []\n",
    "            for token_idx in X_sequence:\n",
    "                X_sequence_embeddings.extend(index_2_embedding[token_idx])\n",
    "\n",
    "            X_batch_embeddings.append(X_sequence_embeddings)\n",
    "\n",
    "        # represent labels as one hot vectors \n",
    "        # resulting shape is (batch_size, |V|) (vocab size is the length of the index -> embedding dictionary)\n",
    "        y_batch = to_categorical(y[i:i+num_sequences_per_batch], num_classes=len(index_2_embedding))\n",
    "\n",
    "        # yield statement instead of return for generator \n",
    "        yield(np.array(X_batch_embeddings), np.array(y_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character data X shape: (128, 200)\n",
      "Character data y shape: (128, 61)\n",
      "\n",
      "Word data X shape: (128, 200)\n",
      "Word data y shape: (128, 25375)\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# initialize your data_generator for both word and character data\n",
    "# print out the shapes of the first batch to verify that it is correct for both word and character data\n",
    "\n",
    "# Examples:\n",
    "num_sequences_per_batch = 128 # this is the batch size\n",
    "#steps_per_epoch = len(sequence)//num_sequences_per_batch  # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 200)\n",
    "# sample[1].shape   # (batch_size, |V|) to_categorical\n",
    "\n",
    "char_data_generator = data_generator(X_char, y_char, num_sequences_per_batch, index_to_embedding_chars)\n",
    "word_data_generator = data_generator(X_word, y_word, num_sequences_per_batch, index_to_embedding_words)\n",
    "\n",
    "char_sample = next(char_data_generator)\n",
    "print(\"Character data X shape:\", char_sample[0].shape)\n",
    "print(\"Character data y shape:\", char_sample[1].shape)\n",
    "\n",
    "print()\n",
    "\n",
    "word_sample = next(word_data_generator)\n",
    "print(\"Word data X shape:\", word_sample[0].shape)\n",
    "print(\"Word data y shape:\", word_sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Train & __save__ your models (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 points \n",
    "\n",
    "# code to train a feedforward neural language model for \n",
    "# both word embeddings and character embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "# (define functions as needed)\n",
    "\n",
    "# train your models for between 3 & 5 epochs\n",
    "# on Felix's machine, this takes ~ 24 min for character embeddings and ~ 10 min for word embeddings\n",
    "# DO NOT EXPECT ACCURACIES OVER 0.5 (and even that is very for this many epochs)\n",
    "# We recommend starting by training for 1 epoch\n",
    "\n",
    "# Define your model architecture using Keras Sequential API\n",
    "# Use the adam optimizer instead of sgd\n",
    "# add cells as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some example code to train a model with a data generator\n",
    "# model.fit(x=train_generator, \n",
    "#           steps_per_epoch=steps_per_epoch,\n",
    "#           epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_neural_net(X_train: np.array, y_train: np.array, num_sequences_per_batch: int, index_2_embedding: dict, num_epochs: int=1, verbose: bool=False):\n",
    "    \"\"\"\n",
    "    Creates and trains a feedforward neural network using given training data.\n",
    "    Neural Network uses 1 hidden layer with 100 hidden units.\n",
    "    Args:\n",
    "        X_train (list of list): featurized training data\n",
    "        y_train (list): training data labels\n",
    "        num_sequences_per_batch (int): batch size for training data \n",
    "        index_2_embedding (dict): mapping from token index -> word2vec embeddings \n",
    "        num_epochs (int): number of training epochs\n",
    "        verbose (bool): if epoch training progress should be printed\n",
    "    Returns:\n",
    "        a trained Neural Network model\n",
    "    \"\"\"\n",
    "    # define model parameters\n",
    "    hidden_units = 100\n",
    "    input_dim = (NGRAM - 1) * EMBEDDINGS_SIZE * 2 # TODO check math here \n",
    "    output_dim = len(index_2_embedding)\n",
    "\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "\n",
    "    # hidden layer \n",
    "    model.add(Dense(units=hidden_units, activation='relu', input_dim=input_dim))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(units=output_dim, activation='sigmoid'))\n",
    "\n",
    "    # configure the learning process\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    for _ in range(num_epochs):\n",
    "        steps_per_epoch = len(X_word)//num_sequences_per_batch\n",
    "        train_generator = data_generator(X_train, y_train, num_sequences_per_batch, index_2_embedding)\n",
    "\n",
    "        # train model \n",
    "        model.fit(x=train_generator, steps_per_epoch=steps_per_epoch, verbose=verbose)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 25374)             2562774   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2582874 (9.85 MB)\n",
      "Trainable params: 2582874 (9.85 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4953/4953 [==============================] - 205s 41ms/step - loss: 5.7324 - accuracy: 0.1924\n",
      "4953/4953 [==============================] - 258s 52ms/step - loss: 5.1709 - accuracy: 0.2060\n",
      "4953/4953 [==============================] - 262s 53ms/step - loss: 4.9393 - accuracy: 0.2083\n",
      "4953/4953 [==============================] - 278s 56ms/step - loss: 4.7588 - accuracy: 0.2095\n",
      "4953/4953 [==============================] - 263s 53ms/step - loss: 4.6191 - accuracy: 0.2106\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# spooky data model by character for 5 epochs takes ~ 24 min on Felix's computer\n",
    "# with adam optimizer, gets accuracy of 0.3920\n",
    "\n",
    "# spooky data model by word for 5 epochs takes 10 min on Felix's computer\n",
    "# results in accuracy of 0.2110\n",
    "\n",
    "\n",
    "word_nn_model = feedforward_neural_net(X_word, y_word, num_sequences_per_batch, index_to_embedding_words, num_epochs=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 100)               20100     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 61)                6161      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26261 (102.58 KB)\n",
      "Trainable params: 26261 (102.58 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4953/4953 [==============================] - 28s 6ms/step - loss: 2.2096 - accuracy: 0.3527\n",
      "4953/4953 [==============================] - 34s 7ms/step - loss: 2.0587 - accuracy: 0.3805\n",
      "4953/4953 [==============================] - 31s 6ms/step - loss: 2.0228 - accuracy: 0.3850\n",
      "4953/4953 [==============================] - 27s 6ms/step - loss: 2.0044 - accuracy: 0.3871\n",
      "4953/4953 [==============================] - 29s 6ms/step - loss: 1.9933 - accuracy: 0.3890\n"
     ]
    }
   ],
   "source": [
    "char_nn_model = feedforward_neural_net(X_char, y_char, num_sequences_per_batch, index_to_embedding_chars, num_epochs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: word_neural_lm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: word_neural_lm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: char_neural_lm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: char_neural_lm\\assets\n"
     ]
    }
   ],
   "source": [
    "# save your trained models so you can re-load instead of re-training each time\n",
    "# also, you'll need these to generate your sentences!\n",
    "word_nn_model.save(\"word_neural_lm\")\n",
    "char_nn_model.save(\"char_neural_lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Generate Sentences (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your models if you need to\n",
    "word_neural_lm = keras.saving.load_model(\"word_neural_lm\")\n",
    "char_neural_lm = keras.saving.load_model(\"char_neural_lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# # generate a sequence from the model until you get an end of sentence token\n",
    "# This is an example function header you might use\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 seed: list):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        seed: [w1, w2, w(n-1)]\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 points\n",
    "\n",
    "# generate and display one sequence from both the word model and the character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "\n",
    "# you may leave _ as _ or replace it with a space if you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 100 example sentences with each model and save them to a file, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
