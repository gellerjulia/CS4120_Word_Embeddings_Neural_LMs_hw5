{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 5: Neural Language Models  (& ðŸŽƒ SpOoKy ðŸ‘» authors ðŸ§Ÿ data) - Task 3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Feedforward Neural Language Model (60 points)\n",
    "--------------------------\n",
    "\n",
    "For this task, you will create and train neural LMs for both your word-based embeddings and your character-based ones. You should write functions when appropriate to avoid excessive copy+pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) First, encode  your text into integers (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shaem\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing utility functions from Keras\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# necessary\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# optional\n",
    "# from keras.layers import Dropout\n",
    "\n",
    "# if you want fancy progress bars\n",
    "from tqdm import notebook\n",
    "from IPython.display import display\n",
    "\n",
    "# your other imports here\n",
    "import time\n",
    "import neurallm_utils as nutils\n",
    "from gensim.models import KeyedVectors # imported by me \n",
    "\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants you may find helpful. Edit as you would like.\n",
    "EMBEDDINGS_SIZE = 50\n",
    "NGRAM = 3 # The ngram language model you want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in necessary data\n",
    "TRAIN_FILE = 'spooky_author_train.csv'\n",
    "char_texts = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=True)\n",
    "word_texts = nutils.read_file_spooky(TRAIN_FILE, NGRAM, by_character=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Tokenizer and fit on your data\n",
    "# do this for both the word and character data\n",
    "\n",
    "# It is used to vectorize a text corpus. Here, it just creates a mapping from \n",
    "# word to a unique index. (Note: Indexing starts from 0)\n",
    "# Example:\n",
    "# tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(data)\n",
    "# encoded = tokenizer.texts_to_sequences(data)\n",
    "char_tokenizer = Tokenizer()\n",
    "char_tokenizer.fit_on_texts(char_texts)\n",
    "encoded_chars = char_tokenizer.texts_to_sequences(char_texts)\n",
    "\n",
    "word_tokenizer = Tokenizer()\n",
    "word_tokenizer.fit_on_texts(word_texts)\n",
    "encoded_words = word_tokenizer.texts_to_sequences(word_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size for character embeddings: 60\n",
      "Vocab size for word embeddings: 25374\n"
     ]
    }
   ],
   "source": [
    "# print out the size of the word index for each of your tokenizers\n",
    "# this should match what you calculated in Task 2 with your embeddings\n",
    "\n",
    "# Vocabulary size for character embeddings is 60\n",
    "# Vocabulary size for word embeddings is 25374\n",
    "print('Vocab size for character embeddings:', len(char_tokenizer.word_index))\n",
    "print('Vocab size for word embeddings:', len(word_tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Next, prepare the sequences to train your model from text (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fixed n-gram based sequences"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The training samples will be structured in the following format. \n",
    "Depening on which ngram model we choose, there will be (n-1) tokens \n",
    "in the input sequence (X) and we will need to predict the nth token (Y)\n",
    "\n",
    "            X,\t\t\t\t\t\t                      y\n",
    "    this,    process                                    however\n",
    "    process, however                                    afforded\n",
    "    however, afforded\t                                me\n",
    "\n",
    "\n",
    "Our first step is to translate the text into sequences of numbers, \n",
    "one sequence per n-gram window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character ngram training samples:\n",
      "Total sequences: 2957553\n",
      "[21, 21, 3]\n",
      "[21, 3, 9]\n",
      "[3, 9, 7]\n",
      "[9, 7, 8]\n",
      "[7, 8, 1]\n",
      "\n",
      "Word ngram training samples:\n",
      "Total sequences: 634080\n",
      "[1, 1, 32]\n",
      "[1, 32, 2956]\n",
      "[32, 2956, 3]\n",
      "[2956, 3, 155]\n",
      "[3, 155, 3]\n"
     ]
    }
   ],
   "source": [
    "def generate_ngram_training_samples(encoded: list, ngram: int) -> list:\n",
    "    '''\n",
    "    Takes the encoded data (list of lists) and \n",
    "    generates the ngram training samples out of it.\n",
    "    Parameters:\n",
    "    encoded: a list of lists produced by kera's tokenizer mapping tokens to unique indices \n",
    "    ngram: the size of the ngrams that should be produced \n",
    "    return: \n",
    "    list of lists in the format [[x1, x2, ... , x(n-1), y], ...]\n",
    "    '''\n",
    "    ngram_samples = []\n",
    "\n",
    "    for line in encoded:\n",
    "        for i in range(len(line) - ngram + 1):\n",
    "            ngram_samples.append(line[i:i+ngram])\n",
    "            \n",
    "    return ngram_samples\n",
    "\n",
    "\n",
    "# generate your training samples for both word and character data\n",
    "# print out the first 5 training samples for each\n",
    "# we have displayed the number of sequences\n",
    "# to expect for both characters and words\n",
    "#\n",
    "# Spooky data by character should give 2957553 sequences\n",
    "# [21, 21, 3]\n",
    "# [21, 3, 9]\n",
    "# [3, 9, 7]\n",
    "# ...\n",
    "# Spooky data by words shoud give 634080 sequences\n",
    "# [1, 1, 32]\n",
    "# [1, 32, 2956]\n",
    "# [32, 2956, 3]\n",
    "# ...\n",
    "\n",
    "char_ngrams = generate_ngram_training_samples(encoded_chars, NGRAM)\n",
    "word_ngrams = generate_ngram_training_samples(encoded_words, NGRAM)\n",
    "\n",
    "print(\"Character ngram training samples:\")\n",
    "print(\"Total sequences:\", len(char_ngrams))\n",
    "for i in range(5):\n",
    "    print(char_ngrams[i])\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Word ngram training samples:\")\n",
    "print(\"Total sequences:\", len(word_ngrams))\n",
    "for i in range(5):\n",
    "    print(word_ngrams[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Then, split the sequences into X and y and create a Data Generator (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X data for words: (634080, 2)\n",
      "Shape of y data for words: (634080,)\n",
      "\n",
      "Shape of X data for characters: (2957553, 2)\n",
      "Shape of y data for characters: (2957553,)\n"
     ]
    }
   ],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Note here that the sequences were in the form: \n",
    "# sequence = [x1, x2, ... , x(n-1), y]\n",
    "# We still need to separate it into [[x1, x2, ... , x(n-1)], ...], [y1, y2, ...]]\n",
    "# do that here\n",
    "def split_ngrams(ngrams: list) -> (np.array, np.array):\n",
    "    \"\"\"\n",
    "    Separate ngram sequences into lists of X and y data.\n",
    "    Args:\n",
    "    ngrams (list of lists): sequences of ngrams in the form of [[x1, x2, ... , x(n-1), y], ...]\n",
    "\n",
    "    Returns:\n",
    "    X (2-D numpy array): sequences of the first n-1 tokens in the ngrams [[x1, x2, ... , x(n-1)], ...]\n",
    "    y (1-D numpy array): list of all the labels aka the n token in the ngrams [y1, y2, ...]\n",
    "    \"\"\"\n",
    "    X =  np.array([ngram[:-1] for ngram in ngrams])\n",
    "    y = np.array([ngram[-1] for ngram in ngrams])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_char, y_char = split_ngrams(char_ngrams)\n",
    "X_word, y_word = split_ngrams(word_ngrams)\n",
    "\n",
    "# print out the shapes to verify that they are correct\n",
    "print(\"Shape of X data for characters:\", X_char.shape)\n",
    "print(\"Shape of y data for characters:\", y_char.shape)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Shape of X data for words:\", X_word.shape)\n",
    "print(\"Shape of y data for words:\", y_word.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5 points\n",
    "\n",
    "# Initialize a function that reads the word embeddings you saved earlier\n",
    "# and gives you back mappings from words to their embeddings and also \n",
    "# indexes from the tokenizers to their embeddings\n",
    "\n",
    "def read_embeddings(filename: str, tokenizer: Tokenizer) -> (dict, dict):\n",
    "    '''Loads and parses embeddings trained in earlier.\n",
    "    Parameters:\n",
    "        filename (str): path to file\n",
    "        Tokenizer: tokenizer used to tokenize the data (needed to get the word to index mapping)\n",
    "    Returns:\n",
    "        (dict): mapping from word to its embedding vector\n",
    "        (dict): mapping from index to its embedding vector\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    # word2vec maps tokens to embedding vectors \n",
    "    word2vec_embeddings = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "\n",
    "    # initialize dictionaries \n",
    "    token_to_embedding = {}\n",
    "    index_to_embedding = {}\n",
    "\n",
    "    # tokenizer maps tokens to unique indices \n",
    "    for token, index in tokenizer.word_index.items():\n",
    "        embedding = word2vec_embeddings[token]\n",
    "\n",
    "        token_to_embedding[token] = embedding\n",
    "        index_to_embedding[index] = embedding\n",
    "\n",
    "    return (token_to_embedding, index_to_embedding)\n",
    "\n",
    "\n",
    "token_to_embedding_words, index_to_embedding_words = read_embeddings(\"spooky_embedding_word.txt\", word_tokenizer)\n",
    "token_to_embedding_chars, index_to_embedding_chars = read_embeddings(\"spooky_embedding_char.txt\", char_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NECESSARY FOR CHARACTERS\n",
    "\n",
    "# the \"0\" index of the Tokenizer is assigned for the padding token. Initialize\n",
    "# the vector for padding token as all zeros of embedding size\n",
    "# this adds one to the number of embeddings that were initially saved\n",
    "# (and increases your vocab size by 1)\n",
    "\n",
    "index_to_embedding_words[0] = [0] * EMBEDDINGS_SIZE\n",
    "index_to_embedding_chars[0] = [0] * EMBEDDINGS_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "\n",
    "def data_generator(X: list, y: list, num_sequences_per_batch: int, index_2_embedding: dict) -> (np.array, np.array):\n",
    "    '''\n",
    "    Returns data generator to be used by feed_forward\n",
    "    https://wiki.python.org/moin/Generators\n",
    "    https://realpython.com/introduction-to-python-generators/\n",
    "    \n",
    "    Yields batches of embeddings and labels to go with them.\n",
    "    Use one hot vectors to encode the labels \n",
    "    (see the to_categorical function)\n",
    "\n",
    "    Args:\n",
    "    X (2-D numpy array): sequences of the first n-1 token indices from training data ngrams [[x1, x2, ... , x(n-1)], ...]\n",
    "    y (1-D numpy array): list of all the labels aka the nth token index from training data ngrams [y1, y2, ...]\n",
    "    num_sequences_per_batch (int): batch size yielded on each iteration of the generator \n",
    "    index_2_embedding (dict): mapping between unique token indices and dense word embeddings \n",
    "\n",
    "    Returns:\n",
    "    X_batch_embeddings (2-D numpy array): sequences of embeddings in the form [[x1_word_embedding ... x2_word_embedding ... x(n-1)_word_embedding], ...]\n",
    "    y_batch (2-D numpy array): a list of one hot vectors encoding labels in the form [y1_one_hot_vector, y2_one_hot_vector, ...]\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # iterate over X and y in batches - stored in the form of unique token indices \n",
    "    for i in range(0, len(X), num_sequences_per_batch):\n",
    "        X_batch = X[i:i+num_sequences_per_batch]\n",
    "\n",
    "        # represents embeddings for each n-1 gram sequence in X_batch \n",
    "        # flattened so resulting shape is (batch_size, (n-1)*EMBEDDING_SIZE)\n",
    "        X_batch_embeddings = []\n",
    "        for X_sequence in X_batch:\n",
    "            # embeddings for a single training sequence / n-1 gram - concatenated to have length (n-1)*EMBEDDING_SIZE\n",
    "            X_sequence_embeddings = []\n",
    "            for token_idx in X_sequence:\n",
    "                X_sequence_embeddings.extend(index_2_embedding[token_idx])\n",
    "\n",
    "            X_batch_embeddings.append(X_sequence_embeddings)\n",
    "\n",
    "        # represent labels as one hot vectors \n",
    "        # resulting shape is (batch_size, |V|) (vocab size is the length of the index -> embedding dictionary)\n",
    "        y_batch = to_categorical(y[i:i+num_sequences_per_batch], num_classes=len(index_2_embedding))\n",
    "\n",
    "        # yield statement instead of return for generator \n",
    "        yield(np.array(X_batch_embeddings), np.array(y_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character data X shape: (128, 100)\n",
      "Character data y shape: (128, 61)\n",
      "\n",
      "Word data X shape: (128, 100)\n",
      "Word data y shape: (128, 25375)\n"
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# initialize your data_generator for both word and character data\n",
    "# print out the shapes of the first batch to verify that it is correct for both word and character data\n",
    "\n",
    "# Examples:\n",
    "num_sequences_per_batch = 128 # this is the batch size\n",
    "#steps_per_epoch = len(sequence)//num_sequences_per_batch  # Number of batches per epoch\n",
    "# train_generator = data_generator(X, y, num_sequences_per_batch)\n",
    "\n",
    "# sample=next(train_generator) # this is how you get data out of generators\n",
    "# sample[0].shape # (batch_size, (n-1)*EMBEDDING_SIZE)  (128, 100)\n",
    "# sample[1].shape   # (batch_size, |V|) to_categorical\n",
    "\n",
    "char_data_generator = data_generator(X_char, y_char, num_sequences_per_batch, index_to_embedding_chars)\n",
    "word_data_generator = data_generator(X_word, y_word, num_sequences_per_batch, index_to_embedding_words)\n",
    "\n",
    "char_sample = next(char_data_generator)\n",
    "print(\"Character data X shape:\", char_sample[0].shape)\n",
    "print(\"Character data y shape:\", char_sample[1].shape)\n",
    "\n",
    "print()\n",
    "\n",
    "word_sample = next(word_data_generator)\n",
    "print(\"Word data X shape:\", word_sample[0].shape)\n",
    "print(\"Word data y shape:\", word_sample[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) Train & __save__ your models (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 points \n",
    "\n",
    "# code to train a feedforward neural language model for \n",
    "# both word embeddings and character embeddings\n",
    "# make sure not to just copy + paste to train your two models\n",
    "# (define functions as needed)\n",
    "\n",
    "# train your models for between 3 & 5 epochs\n",
    "# on Felix's machine, this takes ~ 24 min for character embeddings and ~ 10 min for word embeddings\n",
    "# DO NOT EXPECT ACCURACIES OVER 0.5 (and even that is very for this many epochs)\n",
    "# We recommend starting by training for 1 epoch\n",
    "\n",
    "# Define your model architecture using Keras Sequential API\n",
    "# Use the adam optimizer instead of sgd\n",
    "# add cells as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is some example code to train a model with a data generator\n",
    "# model.fit(x=train_generator, \n",
    "#           steps_per_epoch=steps_per_epoch,\n",
    "#           epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_neural_net(X_train: np.array, \n",
    "                           y_train: np.array, \n",
    "                           num_sequences_per_batch: int, \n",
    "                           index_2_embedding: dict, \n",
    "                           num_epochs: int=1,\n",
    "                           n: int=NGRAM, \n",
    "                           embedding_size: int=EMBEDDINGS_SIZE, \n",
    "                           verbose: bool=False):\n",
    "    \"\"\"\n",
    "    Creates and trains a feedforward neural network using given training data.\n",
    "    Neural Network uses 1 hidden layer with 100 hidden units.\n",
    "    Args:\n",
    "        X_train (list of list): featurized training data\n",
    "        y_train (list): training data labels\n",
    "        num_sequences_per_batch (int): batch size for training data \n",
    "        index_2_embedding (dict): mapping from token index -> word2vec embeddings \n",
    "        num_epochs (int): number of training epochs\n",
    "        n (int): n-gram size used in the training data\n",
    "        embedding_size (int): size of the dense word embeddings used for X_train\n",
    "        verbose (bool): if epoch training progress should be printed\n",
    "    Returns:\n",
    "        a trained Neural Network model\n",
    "    \"\"\"\n",
    "    # define model parameters\n",
    "    hidden_units = 100\n",
    "    hidden_input_dim = (n - 1) * embedding_size     # shape[1] of the X embedding data produced by our generator  \n",
    "    output_dim = len(index_2_embedding)             # vocab size \n",
    "\n",
    "    # instantiate model\n",
    "    model = Sequential()\n",
    "\n",
    "    # hidden layer \n",
    "    model.add(Dense(units=hidden_units, activation='relu', input_dim=hidden_input_dim))\n",
    "\n",
    "    # output layer\n",
    "    model.add(Dense(units=output_dim, activation='softmax'))\n",
    "\n",
    "    # configure the learning process\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    # total number of batches per epoch \n",
    "    steps_per_epoch = len(X_train)//num_sequences_per_batch\n",
    "\n",
    "    for _ in range(num_epochs):\n",
    "        # create a new data generator for us to iterate through\n",
    "        train_generator = data_generator(X_train, y_train, num_sequences_per_batch, index_2_embedding)\n",
    "\n",
    "        # train model \n",
    "        model.fit(x=train_generator, steps_per_epoch=steps_per_epoch, verbose=verbose)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 25375)             2562875   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2572975 (9.82 MB)\n",
      "Trainable params: 2572975 (9.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4953/4953 [==============================] - 293s 59ms/step - loss: 5.7485 - accuracy: 0.1914\n",
      "4953/4953 [==============================] - 284s 57ms/step - loss: 5.1904 - accuracy: 0.2051\n",
      "4953/4953 [==============================] - 287s 58ms/step - loss: 4.9648 - accuracy: 0.2079\n",
      "4953/4953 [==============================] - 293s 59ms/step - loss: 4.7936 - accuracy: 0.2093\n",
      "4953/4953 [==============================] - 283s 57ms/step - loss: 4.6603 - accuracy: 0.2103\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# spooky data model by character for 5 epochs takes ~ 24 min on Felix's computer\n",
    "# with adam optimizer, gets accuracy of 0.3920\n",
    "\n",
    "# spooky data model by word for 5 epochs takes 10 min on Felix's computer\n",
    "# results in accuracy of 0.2110\n",
    "\n",
    "\n",
    "word_nn_model = feedforward_neural_net(X_word, y_word, num_sequences_per_batch, index_to_embedding_words, num_epochs=5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 61)                6161      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16261 (63.52 KB)\n",
      "Trainable params: 16261 (63.52 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23105/23105 [==============================] - 98s 4ms/step - loss: 2.0882 - accuracy: 0.3725\n",
      "23105/23105 [==============================] - 101s 4ms/step - loss: 1.9912 - accuracy: 0.3882\n",
      "23105/23105 [==============================] - 100s 4ms/step - loss: 1.9749 - accuracy: 0.3898\n",
      "23105/23105 [==============================] - 111s 5ms/step - loss: 1.9674 - accuracy: 0.3903\n",
      "23105/23105 [==============================] - 98s 4ms/step - loss: 1.9630 - accuracy: 0.3905\n"
     ]
    }
   ],
   "source": [
    "char_nn_model = feedforward_neural_net(X_char, y_char, num_sequences_per_batch, index_to_embedding_chars, num_epochs=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: word_neural_lm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: word_neural_lm\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: char_neural_lm\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: char_neural_lm\\assets\n"
     ]
    }
   ],
   "source": [
    "# save your trained models so you can re-load instead of re-training each time\n",
    "# also, you'll need these to generate your sentences!\n",
    "word_nn_model.save(\"word_neural_lm\")\n",
    "char_nn_model.save(\"char_neural_lm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Generate Sentences (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your models if you need to\n",
    "word_neural_lm = keras.saving.load_model(\"word_neural_lm\")\n",
    "char_neural_lm = keras.saving.load_model(\"char_neural_lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 points\n",
    "\n",
    "# # generate a sequence from the model until you get an end of sentence token\n",
    "# This is an example function header you might use\n",
    "def generate_seq(model: Sequential, \n",
    "                 tokenizer: Tokenizer, \n",
    "                 index_2_embedding: dict, \n",
    "                 seed: list):\n",
    "    '''\n",
    "    Parameters:\n",
    "        model: your neural network\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        seed: [w1, w2, w(n-1)]\n",
    "    Returns: string sentence\n",
    "    '''\n",
    "    sentence_begin_index = tokenizer.word_index.get(nutils.SENTENCE_BEGIN)\n",
    "    sentence_end_index = tokenizer.word_index.get(nutils.SENTENCE_END)\n",
    "\n",
    "    # track the unique token indices for the sequence \n",
    "    sequence_indices = [tokenizer.word_index.get(tok) for tok in seed] \n",
    "\n",
    "    nn_input_length = len(seed)\n",
    "    # until we get a SENTENCE_END token\n",
    "    while sequence_indices[-1] != sentence_end_index:\n",
    "        # get the latest n-1 token indices \n",
    "        input_sequence = sequence_indices[-1*nn_input_length:]\n",
    "\n",
    "        # convert the input sequence to embeddings (concatenated together)\n",
    "        input_embeddings = []\n",
    "        for idx in input_sequence:\n",
    "            input_embeddings.extend(index_2_embedding[idx])\n",
    "\n",
    "        # convert to numpy array\n",
    "        input_embeddings = np.array([input_embeddings])\n",
    "\n",
    "        # get probability distribution on vocabulary for the next token in the sequence \n",
    "        distribution = model.predict(input_embeddings, verbose=False)\n",
    "        next_tok_idx = np.argmax(distribution)\n",
    "\n",
    "        # skip mid-sentence SENTENCE_BEGIN tokens\n",
    "        if next_tok_idx == sentence_begin_index:\n",
    "            continue\n",
    "        \n",
    "        sequence_indices.append(next_tok_idx)\n",
    "\n",
    "    sequence = [tokenizer.word_index[idx] for idx in sequence_indices] \n",
    "    return ' '.join(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(model: Sequential, \n",
    "                      tokenizer: Tokenizer, \n",
    "                      index_2_embedding: dict, \n",
    "                      num_seq: int,\n",
    "                      by_char: bool,\n",
    "                      n: int=NGRAM):\n",
    "    '''\n",
    "    Generates a given number of sequences using the given neural network language model.\n",
    "    Will begin the sequence generation with n-1 SENTENCE_BEGIN tokens.\n",
    "    Returned sequences will have the BEGIN and END tokens removed\n",
    "    For character models, _ will be replaced with spaces  \n",
    "\n",
    "    Parameters:\n",
    "        model: neural network language model\n",
    "        tokenizer: the keras preprocessing tokenizer\n",
    "        index_2_embedding: mapping from token index -> word2vec embeddings \n",
    "        num_seq: the number of sequences to generate \n",
    "        by_char: True if a character model, False if a word model is used \n",
    "        n: the size of the ngram used to train the neural network model\n",
    "\n",
    "    Returns: \n",
    "        a list of strings, where each string is a generated sequence with <s> or </s> tokens removed \n",
    "    '''\n",
    "    seed = [nutils.SENTENCE_BEGIN] * (NGRAM - 1)\n",
    "\n",
    "    sequences = []\n",
    "    for _ in range(num_seq):\n",
    "        seq = generate_seq(model, tokenizer, index_2_embedding, seed)\n",
    "        seq = seq.replace(nutils.SENTENCE_BEGIN, '')\n",
    "        seq = seq.replace(nutils.SENTENCE_END, '')\n",
    "        if by_char:\n",
    "            seq = seq.replace('_', ' ')\n",
    "        sequences.append(seq.strip())\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "38\n",
      "50\n",
      "4\n",
      "95\n",
      "81\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n",
      "4\n",
      "180\n",
      "3\n",
      "7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\shaem\\NLP\\HW\\CS4120_Word_Embeddings_Neural_LMs_hw5\\neurallm_task3.ipynb Cell 30\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# 5 points\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# generate and display one sequence from both the word model and the character model\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# you may leave _ as _ or replace it with a space if you prefer\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(generate_sequences(word_neural_lm, word_tokenizer, index_to_embedding_words, num_seq\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, by_char\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m))\n",
      "\u001b[1;32mc:\\Users\\shaem\\NLP\\HW\\CS4120_Word_Embeddings_Neural_LMs_hw5\\neurallm_task3.ipynb Cell 30\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m sequences \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_seq):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     seq \u001b[39m=\u001b[39m generate_seq(model, tokenizer, index_2_embedding, seed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     seq \u001b[39m=\u001b[39m seq\u001b[39m.\u001b[39mreplace(nutils\u001b[39m.\u001b[39mSENTENCE_BEGIN, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     seq \u001b[39m=\u001b[39m seq\u001b[39m.\u001b[39mreplace(nutils\u001b[39m.\u001b[39mSENTENCE_END, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\shaem\\NLP\\HW\\CS4120_Word_Embeddings_Neural_LMs_hw5\\neurallm_task3.ipynb Cell 30\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m input_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([input_embeddings])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# get probability distribution on vocabulary for the next token in the sequence \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m distribution \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(input_embeddings, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m next_tok_idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(distribution)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/shaem/NLP/HW/CS4120_Word_Embeddings_Neural_LMs_hw5/neurallm_task3.ipynb#X35sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mprint\u001b[39m(next_tok_idx)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py:2596\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2587\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   2588\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2589\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2590\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2593\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2594\u001b[0m         )\n\u001b[1;32m-> 2596\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   2597\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   2598\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2599\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   2600\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   2601\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2602\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2603\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2604\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2605\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2606\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[0;32m   2607\u001b[0m )\n\u001b[0;32m   2609\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2610\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1688\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m         \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1687\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1688\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1292\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[0;32m   1291\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1293\u001b[0m     x,\n\u001b[0;32m   1294\u001b[0m     y,\n\u001b[0;32m   1295\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1296\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1297\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1298\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1299\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1300\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1301\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1302\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1303\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1304\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1305\u001b[0m     pss_evaluation_shards\u001b[39m=\u001b[39;49mpss_evaluation_shards,\n\u001b[0;32m   1306\u001b[0m )\n\u001b[0;32m   1308\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1310\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:353\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         flat_dataset \u001b[39m=\u001b[39m flat_dataset\u001b[39m.\u001b[39mshuffle(\u001b[39m1024\u001b[39m)\u001b[39m.\u001b[39mrepeat(epochs)\n\u001b[0;32m    351\u001b[0m     \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[1;32m--> 353\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39;49mflat_map(slice_batch_indices)\n\u001b[0;32m    355\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    357\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2313\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[1;34m(self, map_func, name)\u001b[0m\n\u001b[0;32m   2309\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops -> flat_map_op ->\u001b[39;00m\n\u001b[0;32m   2310\u001b[0m \u001b[39m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2311\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2312\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m flat_map_op\n\u001b[1;32m-> 2313\u001b[0m \u001b[39mreturn\u001b[39;00m flat_map_op\u001b[39m.\u001b[39;49m_flat_map(\u001b[39mself\u001b[39;49m, map_func, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\flat_map_op.py:24\u001b[0m, in \u001b[0;36m_flat_map\u001b[1;34m(input_dataset, map_func, name)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_flat_map\u001b[39m(input_dataset, map_func, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m   \u001b[39mreturn\u001b[39;00m _FlatMapDataset(input_dataset, map_func, name)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\flat_map_op.py:33\u001b[0m, in \u001b[0;36m_FlatMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, input_dataset, map_func, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[1;32m---> 33\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[0;32m     34\u001b[0m       map_func, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(), dataset\u001b[39m=\u001b[39;49minput_dataset)\n\u001b[0;32m     35\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure, dataset_ops\u001b[39m.\u001b[39mDatasetSpec):\n\u001b[0;32m     36\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m     37\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdataset_ops\u001b[39m.\u001b[39mget_type(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func\u001b[39m.\u001b[39moutput_structure)\u001b[39m!r}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    259\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    260\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    263\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 265\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[0;32m    266\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1222\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1220\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1221\u001b[0m   \u001b[39m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[1;32m-> 1222\u001b[0m   concrete \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_concrete_function_garbage_collected(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1223\u001b[0m   concrete\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1192\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1191\u001b[0m     initializers \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 1192\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwargs, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[0;32m   1193\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[0;32m   1195\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[0;32m   1196\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m   1197\u001b[0m   \u001b[39m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:694\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    689\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    690\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    691\u001b[0m     tracing_compilation\u001b[39m.\u001b[39mScopeType\u001b[39m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    692\u001b[0m )\n\u001b[0;32m    693\u001b[0m \u001b[39m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 694\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m tracing_compilation\u001b[39m.\u001b[39;49mtrace_function(\n\u001b[0;32m    695\u001b[0m     args, kwds, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_config\n\u001b[0;32m    696\u001b[0m )\n\u001b[0;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[0;32m    699\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[39m=\u001b[39m tracing_options\u001b[39m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[39m=\u001b[39m _maybe_define_function(\n\u001b[0;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[0;32m    180\u001b[0m   )\n\u001b[0;32m    181\u001b[0m   _set_arg_keywords(concrete_function)\n\u001b[0;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tracing_options\u001b[39m.\u001b[39mbind_graph_to_function:\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:284\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m   target_func_type \u001b[39m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 284\u001b[0m concrete_function \u001b[39m=\u001b[39m _create_concrete_function(\n\u001b[0;32m    285\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[0;32m    286\u001b[0m )\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m tracing_options\u001b[39m.\u001b[39mfunction_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    289\u001b[0m   tracing_options\u001b[39m.\u001b[39mfunction_cache\u001b[39m.\u001b[39madd(\n\u001b[0;32m    290\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    291\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:308\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[39mwith\u001b[39;00m func_graph\u001b[39m.\u001b[39mas_default():\n\u001b[0;32m    304\u001b[0m   placeholder_bound_args \u001b[39m=\u001b[39m function_type\u001b[39m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    305\u001b[0m       placeholder_context\n\u001b[0;32m    306\u001b[0m   )\n\u001b[1;32m--> 308\u001b[0m traced_func_graph \u001b[39m=\u001b[39m func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[0;32m    309\u001b[0m     tracing_options\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    310\u001b[0m     tracing_options\u001b[39m.\u001b[39;49mpython_function,\n\u001b[0;32m    311\u001b[0m     placeholder_bound_args\u001b[39m.\u001b[39;49margs,\n\u001b[0;32m    312\u001b[0m     placeholder_bound_args\u001b[39m.\u001b[39;49mkwargs,\n\u001b[0;32m    313\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    314\u001b[0m     func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[0;32m    315\u001b[0m     arg_names\u001b[39m=\u001b[39;49mfunction_type_utils\u001b[39m.\u001b[39;49mto_arg_names(function_type),\n\u001b[0;32m    316\u001b[0m     create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    317\u001b[0m )\n\u001b[0;32m    319\u001b[0m transform\u001b[39m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    321\u001b[0m graph_capture_container \u001b[39m=\u001b[39m traced_func_graph\u001b[39m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1056\u001b[0m   \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m   1058\u001b[0m _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1059\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[0;32m   1061\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:597\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    594\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    595\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 597\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    598\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:231\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.trace_tf_function.<locals>.wrapped_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_fn\u001b[39m(\u001b[39m*\u001b[39margs):  \u001b[39m# pylint: disable=missing-docstring\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m   ret \u001b[39m=\u001b[39m wrapper_helper(\u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    232\u001b[0m   ret \u001b[39m=\u001b[39m structure\u001b[39m.\u001b[39mto_tensor_list(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_structure, ret)\n\u001b[0;32m    233\u001b[0m   \u001b[39mreturn\u001b[39;00m [ops\u001b[39m.\u001b[39mconvert_to_tensor(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m ret]\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:161\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__.<locals>.wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _should_unpack(nested_args):\n\u001b[0;32m    160\u001b[0m   nested_args \u001b[39m=\u001b[39m (nested_args,)\n\u001b[1;32m--> 161\u001b[0m ret \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39;49mtf_convert(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func, ag_ctx)(\u001b[39m*\u001b[39;49mnested_args)\n\u001b[0;32m    162\u001b[0m ret \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(ret)\n\u001b[0;32m    163\u001b[0m \u001b[39mif\u001b[39;00m _should_pack(ret):\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[1;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[0;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[1;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:334\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__.<locals>.slice_batch_indices\u001b[1;34m(indices)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Convert a Tensor of indices into a dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \n\u001b[0;32m    319\u001b[0m \u001b[39mThis step can be accomplished in several ways. The most natural is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[39m  A Dataset of batched indices.\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    333\u001b[0m num_in_full_batch \u001b[39m=\u001b[39m num_full_batches \u001b[39m*\u001b[39m batch_size\n\u001b[1;32m--> 334\u001b[0m first_k_indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mslice(indices, [\u001b[39m0\u001b[39;49m], [num_in_full_batch])\n\u001b[0;32m    335\u001b[0m first_k_indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreshape(\n\u001b[0;32m    336\u001b[0m     first_k_indices, [num_full_batches, batch_size]\n\u001b[0;32m    337\u001b[0m )\n\u001b[0;32m    339\u001b[0m flat_dataset \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataset\u001b[39m.\u001b[39mfrom_tensor_slices(first_k_indices)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:1232\u001b[0m, in \u001b[0;36mslice\u001b[1;34m(input_, begin, size, name)\u001b[0m\n\u001b[0;32m   1180\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mslice\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1181\u001b[0m \u001b[39m@dispatch\u001b[39m\u001b[39m.\u001b[39madd_dispatch_support\n\u001b[0;32m   1182\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mslice\u001b[39m(input_, begin, size, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1183\u001b[0m   \u001b[39m# pylint: disable=redefined-builtin\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Extracts a slice from a tensor.\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m \n\u001b[0;32m   1186\u001b[0m \u001b[39m  See also `tf.strided_slice`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1230\u001b[0m \u001b[39m    A `Tensor` the same type as `input_`.\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1232\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39;49m_slice(input_, begin, size, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py:12176\u001b[0m, in \u001b[0;36m_slice\u001b[1;34m(input, begin, size, name)\u001b[0m\n\u001b[0;32m  12174\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m  12175\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m> 12176\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[0;32m  12177\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mSlice\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m, begin\u001b[39m=\u001b[39;49mbegin, size\u001b[39m=\u001b[39;49msize, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m  12178\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[0;32m  12179\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:778\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[39mwith\u001b[39;00m g\u001b[39m.\u001b[39mas_default(), ops\u001b[39m.\u001b[39mname_scope(name) \u001b[39mas\u001b[39;00m scope:\n\u001b[0;32m    777\u001b[0m   \u001b[39mif\u001b[39;00m fallback:\n\u001b[1;32m--> 778\u001b[0m     _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map,\n\u001b[0;32m    779\u001b[0m                            keywords, default_type_attr_map, attrs, inputs,\n\u001b[0;32m    780\u001b[0m                            input_types)\n\u001b[0;32m    781\u001b[0m     _ExtractRemainingAttrs(op_type_name, op_def, keywords,\n\u001b[0;32m    782\u001b[0m                            default_type_attr_map, attrs)\n\u001b[0;32m    783\u001b[0m     _ExtractAttrProto(op_type_name, op_def, attrs, attr_protos)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:551\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[1;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[0;32m    545\u001b[0m       values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m    546\u001b[0m           values,\n\u001b[0;32m    547\u001b[0m           name\u001b[39m=\u001b[39minput_arg\u001b[39m.\u001b[39mname,\n\u001b[0;32m    548\u001b[0m           as_ref\u001b[39m=\u001b[39minput_arg\u001b[39m.\u001b[39mis_ref,\n\u001b[0;32m    549\u001b[0m           preferred_dtype\u001b[39m=\u001b[39mdefault_dtype)\n\u001b[0;32m    550\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 551\u001b[0m     values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mconvert_to_tensor(\n\u001b[0;32m    552\u001b[0m         values,\n\u001b[0;32m    553\u001b[0m         name\u001b[39m=\u001b[39;49minput_arg\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    554\u001b[0m         dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    555\u001b[0m         as_ref\u001b[39m=\u001b[39;49minput_arg\u001b[39m.\u001b[39;49mis_ref,\n\u001b[0;32m    556\u001b[0m         preferred_dtype\u001b[39m=\u001b[39;49mdefault_dtype)\n\u001b[0;32m    557\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    558\u001b[0m   \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[39mwith\u001b[39;00m Trace(trace_name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:698\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[39m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[0;32m    697\u001b[0m preferred_dtype \u001b[39m=\u001b[39m preferred_dtype \u001b[39mor\u001b[39;00m dtype_hint\n\u001b[1;32m--> 698\u001b[0m \u001b[39mreturn\u001b[39;00m tensor_conversion_registry\u001b[39m.\u001b[39;49mconvert(\n\u001b[0;32m    699\u001b[0m     value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[0;32m    700\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m           _add_error_prefix(\n\u001b[0;32m    227\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConversion function \u001b[39m\u001b[39m{\u001b[39;00mconversion_func\u001b[39m!r}\u001b[39;00m\u001b[39m for type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m               \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mactual = \u001b[39m\u001b[39m{\u001b[39;00mret\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mbase_dtype\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m               name\u001b[39m=\u001b[39mname))\n\u001b[0;32m    233\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m   ret \u001b[39m=\u001b[39m conversion_func(value, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname, as_ref\u001b[39m=\u001b[39;49mas_ref)\n\u001b[0;32m    236\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39mis\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[0;32m    237\u001b[0m   \u001b[39mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:328\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    326\u001b[0m                                          as_ref\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    327\u001b[0m   _ \u001b[39m=\u001b[39m as_ref\n\u001b[1;32m--> 328\u001b[0m   \u001b[39mreturn\u001b[39;00m constant(v, dtype\u001b[39m=\u001b[39;49mdtype, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconstant\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[])\n\u001b[0;32m    171\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconstant\u001b[39m(value, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, shape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[39m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    268\u001b[0m                         allow_broadcast\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:281\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    278\u001b[0m       \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    279\u001b[0m   \u001b[39mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 281\u001b[0m const_tensor \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49m_create_graph_constant(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    282\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[0;32m    283\u001b[0m )\n\u001b[0;32m    284\u001b[0m \u001b[39mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:270\u001b[0m, in \u001b[0;36m_create_graph_constant\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    268\u001b[0m dtype_value \u001b[39m=\u001b[39m attr_value_pb2\u001b[39m.\u001b[39mAttrValue(\u001b[39mtype\u001b[39m\u001b[39m=\u001b[39mtensor_value\u001b[39m.\u001b[39mtensor\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m    269\u001b[0m attrs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m\"\u001b[39m: tensor_value, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: dtype_value}\n\u001b[1;32m--> 270\u001b[0m const_tensor \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    271\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mConst\u001b[39;49m\u001b[39m\"\u001b[39;49m, [], [dtype_value\u001b[39m.\u001b[39;49mtype], attrs\u001b[39m=\u001b[39;49mattrs, name\u001b[39m=\u001b[39;49mname)\u001b[39m.\u001b[39moutputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m op_callbacks\u001b[39m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[0;32m    274\u001b[0m   \u001b[39m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[0;32m    275\u001b[0m   \u001b[39m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m   callback_outputs \u001b[39m=\u001b[39m op_callbacks\u001b[39m.\u001b[39minvoke_op_callbacks(\n\u001b[0;32m    277\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mConst\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[39m=\u001b[39mname, graph\u001b[39m=\u001b[39mg)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    668\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[1;32m--> 670\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2657\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   2654\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   2655\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   2656\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 2657\u001b[0m   ret \u001b[39m=\u001b[39m Operation\u001b[39m.\u001b[39;49mfrom_node_def(\n\u001b[0;32m   2658\u001b[0m       node_def,\n\u001b[0;32m   2659\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2660\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[0;32m   2661\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[0;32m   2662\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[0;32m   2663\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m   2664\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[0;32m   2665\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def,\n\u001b[0;32m   2666\u001b[0m   )\n\u001b[0;32m   2667\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[0;32m   2668\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1161\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1158\u001b[0m     control_input_ops\u001b[39m.\u001b[39mappend(control_op)\n\u001b[0;32m   1160\u001b[0m \u001b[39m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1161\u001b[0m c_op \u001b[39m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m   1162\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Operation(c_op, SymbolicTensor)\n\u001b[0;32m   1163\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init(g)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\shaem\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1018\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1014\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[39m.\u001b[39mas_str(name),\n\u001b[0;32m   1015\u001b[0m                                          serialized)\n\u001b[0;32m   1017\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1018\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39;49mTF_FinishOperation(op_desc)\n\u001b[0;32m   1019\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1020\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e\u001b[39m.\u001b[39mmessage)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5 points\n",
    "\n",
    "# generate and display one sequence from both the word model and the character model\n",
    "# do not include <s> or </s> in your displayed sentences\n",
    "# make sure that you can read the output easily (i.e. don't just print out a list of tokens)\n",
    "\n",
    "# you may leave _ as _ or replace it with a space if you prefer\n",
    "print(generate_sequences(word_neural_lm, word_tokenizer, index_to_embedding_words, num_seq=1, by_char=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 100 example sentences with each model and save them to a file, one sentence per line\n",
    "# do not include <s> and </s> in your saved sentences (you'll use these sentences in your next task)\n",
    "# this will produce two files, one for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
